{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPNjxjPV3PW22H36XS0OerZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedamusk/final-year/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install obspy tensorflow numpy pandas matplotlib"
      ],
      "metadata": {
        "id": "uMtynx9Ih_WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrSbrO41VoB4"
      },
      "outputs": [],
      "source": [
        "import obspy\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy import UTCDateTime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "import tensorflow as tf\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SOURCES = [\n",
        "    {\"client\": \"IRIS\", \"network\": \"IU\", \"station\": \"ANMO\", \"location\": \"00\", \"channel\": \"BHZ\"},\n",
        "    {\"client\": \"USGS\", \"network\": \"GS\", \"station\": \"TPNV\", \"location\": \"00\", \"channel\": \"BHZ\"},\n",
        "    {\"client\": \"GEOFON\", \"network\": \"GE\", \"station\": \"RUE\", \"location\": \"\", \"channel\": \"BHZ\"}\n",
        "]\n",
        "\n",
        "def download_and_preprocess_data(start_time, end_time, source):\n",
        "    client = Client(source[\"client\"])\n",
        "    try:\n",
        "        st = client.get_waveforms(source[\"network\"], source[\"station\"], source[\"location\"], source[\"channel\"], start_time, end_time)\n",
        "\n",
        "        st.filter('bandpass', freqmin=1, freqmax=20)\n",
        "\n",
        "        st.trim(starttime=start_time, endtime=end_time)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        normalized_data = scaler.fit_transform(st[0].data.reshape(-1, 1)).flatten()\n",
        "\n",
        "        return normalized_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading data from {source['client']} for {source['station']}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def parallel_download(start_time, end_time, sources):\n",
        "    with ThreadPoolExecutor(max_workers=len(sources)) as executor:\n",
        "        futures = [executor.submit(download_and_preprocess_data, start_time, end_time, source) for source in sources]\n",
        "        results = [future.result() for future in as_completed(futures) if future.result() is not None]\n",
        "\n",
        "    max_len=max(len(result) for result in results)\n",
        "    padded_results=[np.pad(result, (0, max_len-len(result)), 'constant')for result in results]\n",
        "    return np.array(padded_results)\n",
        "\n",
        "\n",
        "\n",
        "def sta_lta(data, nsta, nlta):\n",
        "    sta = np.cumsum(data**2)\n",
        "    sta = np.require(sta, dtype=float)\n",
        "    sta[nsta:] = sta[nsta:] - sta[:-nsta]\n",
        "    sta /= nsta\n",
        "    lta = np.cumsum(data**2)\n",
        "    lta = np.require(lta, dtype=float)\n",
        "    lta[nlta:] = lta[nlta:] - lta[:-nlta]\n",
        "    lta /= nlta\n",
        "    return sta / lta\n",
        "\n",
        "def create_rnn_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.SimpleRNN(64, input_shape=input_shape, return_sequences=True),\n",
        "        tf.keras.layers.SimpleRNN(32),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def integrated_detection(data, sta_lta_threshold, rnn_model, window_size):\n",
        "    sta_lta_ratio = sta_lta(data, nsta=int(0.5*window_size), nlta=int(2*window_size))\n",
        "    detections = []\n",
        "    for i in range(0, len(data)-window_size, window_size):\n",
        "        if max(sta_lta_ratio[i:i+window_size]) > sta_lta_threshold:\n",
        "            window = data[i:i+window_size].reshape(1, window_size, 1)\n",
        "            rnn_prediction = rnn_model.predict(window)[0][0]\n",
        "            if rnn_prediction > 0.5:\n",
        "                detections.append(i)\n",
        "    return detections\n",
        "\n",
        "def evaluate_performance(true_events, detected_events, total_windows):\n",
        "    true_positives = len(set(true_events) & set(detected_events))\n",
        "    false_positives = len(detected_events) - true_positives\n",
        "    false_negatives = len(true_events) - true_positives\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "def load_earthquake_catalog(start_time, end_time):\n",
        "    num_days = (end_time - start_time) / (24 * 3600)\n",
        "    num_events = int(num_days * 5)\n",
        "    time_diff_seconds=(end_time-start_time)\n",
        "    event_times = sorted(start_time + offset for offset in np.random.rand(num_events) * time_diff_seconds)\n",
        "    return pd.DataFrame({'time': event_times, 'magnitude': np.random.uniform(2, 6, num_events)})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = UTCDateTime(\"2023-01-01T00:00:00\")\n",
        "    end_time = UTCDateTime(\"2023-01-02T00:00:00\")  # Changed to one day for quicker execution\n",
        "\n",
        "    print(\"Downloading and preprocessing data...\")\n",
        "    data = parallel_download(start_time, end_time, SOURCES)\n",
        "\n",
        "    print(\"Loading earthquake catalog...\")\n",
        "    catalog = load_earthquake_catalog(start_time, end_time)\n",
        "\n",
        "    window_size = 3600\n",
        "    step_size = 1800\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    print(\"Preparing data for RNN...\")\n",
        "    for station_data in data:\n",
        "        for i in range(0, len(station_data) - window_size, step_size):\n",
        "            window_start = start_time + i * (end_time - start_time) / len(station_data)\n",
        "            window_end = window_start + window_size * (end_time - start_time) / len(station_data)\n",
        "            window = station_data[i:i+window_size]\n",
        "            X.append(window)\n",
        "\n",
        "            y.append(1 if len(catalog[(catalog['time'] >= window_start) & (catalog['time'] < window_end)]) > 0 else 0)\n",
        "\n",
        "    X = np.array(X).reshape(-1, window_size, 1)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"Training RNN model...\")\n",
        "    rnn_model = create_rnn_model((window_size, 1))\n",
        "    rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    print(\"Evaluating RNN model...\")\n",
        "    y_pred = rnn_model.predict(X_test)\n",
        "    rnn_precision, rnn_recall, rnn_f1, _ = precision_recall_fscore_support(y_test, y_pred.round(), average='binary')\n",
        "    rnn_auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print(\"RNN Performance:\")\n",
        "    print(f\"Precision: {rnn_precision:.2f}\")\n",
        "    print(f\"Recall: {rnn_recall:.2f}\")\n",
        "    print(f\"F1 Score: {rnn_f1:.2f}\")\n",
        "    print(f\"AUC: {rnn_auc:.2f}\")\n",
        "\n",
        "    print(\"Evaluating Integrated STA/LTA and RNN...\")\n",
        "    sta_lta_threshold = 2.5\n",
        "    detected_events = []\n",
        "    for station_data in data:\n",
        "        detected_events.extend(integrated_detection(station_data, sta_lta_threshold, rnn_model, window_size))\n",
        "\n",
        "    true_events = [int((event_time - start_time) / (end_time - start_time) * len(data[0]))\n",
        "                   for event_time in catalog['time']]\n",
        "\n",
        "    integrated_precision, integrated_recall, integrated_f1 = evaluate_performance(true_events, detected_events, len(data[0]))\n",
        "\n",
        "    print(\"\\nIntegrated STA/LTA and RNN Performance:\")\n",
        "    print(f\"Precision: {integrated_precision:.2f}\")\n",
        "    print(f\"Recall: {integrated_recall:.2f}\")\n",
        "    print(f\"F1 Score: {integrated_f1:.2f}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(data[0])\n",
        "    plt.title(\"Seismic Data (First Station)\")\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(catalog['time'], catalog['magnitude'], 'ro', markersize=3)\n",
        "    plt.title(\"Earthquake Catalog\")\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel(\"Magnitude\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ]
}