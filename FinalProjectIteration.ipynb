{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNBGMbBEIbNU2E9jQFOM3I3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedamusk/final-year/blob/main/FinalProjectIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVKM8cxB_Zf4"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow obspy matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from obspy import UTCDateTime, Stream, Trace\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy.signal.trigger import classic_sta_lta\n",
        "from obspy.signal.filter import bandpass\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (LSTM, Dense, Dropout, BatchNormalization,\n",
        "                                     Bidirectional, SimpleRNN, GRU, Conv1D, MaxPooling1D)\n",
        "from tensorflow.keras.callbacks import(\n",
        "    EarlyStopping, ReduceLROnPlateau,\n",
        "    ModelCheckpoint\n",
        ")\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix,\n",
        "    accuracy_score, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "4S1Tf_eA_cks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_output_directory():\n",
        "  \"\"\"Create a directory for saving plots and models\"\"\"\n",
        "  base_dir='seismic_detection_outputs'\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "  return base_dir"
      ],
      "metadata": {
        "id": "6Q38pRGADqNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_iris_data(network, station, location, channel, origin_time, duration=120):\n",
        "  \"\"\"Fetch Seismic data from the IRIS database\"\"\"\n",
        "  try:\n",
        "    client=Client(\"IRIS\")\n",
        "    start_time=origin_time\n",
        "    end_time=origin_time+duration\n",
        "\n",
        "    stream=client.get_waveforms(network, station, location, channel, start_time, end_time)\n",
        "    return stream\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching data from IRIS:{str(e)}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "Zl83CUf0E0G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data(num_samples, sample_rate, event_duration, noise_level, num_events=3):\n",
        "  \"\"\"Generate synthetic data with multiple events\"\"\"\n",
        "  time=np.arange(num_samples)/ sample_rate\n",
        "  background=np.random.normal(0, noise_level, num_samples)\n",
        "  events=np.zeros(num_samples)\n",
        "  event_locations=[]\n",
        "\n",
        "  for _ in range(np.random.randint(1, num_events+1)):\n",
        "    event_start=np.random.randint(num_samples // 8, num_samples*7//8)\n",
        "    event_end=event_start + int(event_duration*sample_rate)\n",
        "    event_locations.append((event_start, event_end))\n",
        "\n",
        "    #Create more realistic events with multiple frequency components\n",
        "    freq1=np.random.uniform(3,8)\n",
        "    freq2=np.random.uniform(10, 20)\n",
        "    amp1=np.random.uniform(1.0, 2.0)\n",
        "    amp2=np.random.uniform(0.5, 1.5)\n",
        "    decay1=np.random.uniform(0.1, 0.3)\n",
        "    decay2=np.random.uniform(0.2, 0.4)\n",
        "\n",
        "    event_time=time[event_start:event_end]-time[event_start]\n",
        "    event=(\n",
        "        amp1*np.sin(2*np.pi*freq1*event_time)*np.exp(-event_time/decay1)+\n",
        "        amp2*np.sin(2*np.pi*freq2*event_time)*np.exp(-event_time/decay2)\n",
        "    )\n",
        "    events[event_start:event_end]=event\n",
        "  data=background+events\n",
        "  return data, events, event_locations"
      ],
      "metadata": {
        "id": "TCAo4L3XFqiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_stream(stream, freqmin=0.5, freqmax=20):\n",
        "  \"\"\"Enhanced stream processing with more robust filtering\"\"\"\n",
        "  processed_stream=stream.copy()\n",
        "  processed_stream.detrend('linear')\n",
        "  processed_stream.taper(max_percentage=0.1)\n",
        "  processed_stream.filter('bandpass', freqmin=freqmin, freqmax=freqmax,\n",
        "                          corners=6, zerophase=True)\n",
        "\n",
        "  #Add STA/LTA trigger for additional event detection\n",
        "  trace=processed_stream[0]\n",
        "  cft=classic_sta_lta(trace.data, int(0.5*trace.stats.sampling_rate),\n",
        "                      int(10*trace.stats.sampling_rate))\n",
        "  return processed_stream, cft"
      ],
      "metadata": {
        "id": "BPsZSOXbKrnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_enhanced_windows(data, window_size, step, event_locations=None, cft=None):\n",
        "  \"\"\"Create window with advanced feature engineering and labelling\"\"\"\n",
        "  windows=[]\n",
        "  labels=[]\n",
        "\n",
        "  #add STA/LTA trigger-based event detection if available\n",
        "  def detect_event_by_trigger(window_start, window_end, cft=None):\n",
        "    if cft is not None:\n",
        "      window_cft=cft[window_start:window_end]\n",
        "      return np.max(window_cft)>3.0 #Adjust threshold as needed\n",
        "    return False\n",
        "\n",
        "  for i in range(0, len(data)-window_size+1, step):\n",
        "    window=data[i:i+window_size]\n",
        "    freq_features=np.fft.fft(window)[:window_size//2].real\n",
        "    freq_features_padded=np.pad(freq_features, (0, window_size-len(freq_features)), 'constant')\n",
        "\n",
        "    #Advanced feature engineering\n",
        "    window_features=np.column_stack([\n",
        "        window, #Raw signal\n",
        "        np.abs(window), #Absolute amplitude\n",
        "        np.gradient(window), #First deravitive\n",
        "        np.gradient(np.gradient(window)), #Second derivative\n",
        "        np.log1p(np.abs(window)), #Log of absolute amplitude\n",
        "        freq_features_padded, #Padded frequency domain features\n",
        "    ])\n",
        "\n",
        "    #Labelling logic with multiple detection methods\n",
        "    label=0\n",
        "    if event_locations:\n",
        "      #Check if window contains an event from synthetic data\n",
        "      for start, end in event_locations:\n",
        "        if (i<=end and i +window_size >= start):\n",
        "          label=1\n",
        "          break\n",
        "\n",
        "    #Additional event detection using STA/LTA\n",
        "    if label==0 and detect_event_by_trigger(i, i +window_size, cft):\n",
        "      label=1\n",
        "\n",
        "    #Fall back: statistical anomaly detection\n",
        "    if label==0:\n",
        "      window_mean=np.abs(window).mean()\n",
        "      data_std=np.abs(data).std()\n",
        "      label=1 if window_mean> data_std *2.5 else 0\n",
        "\n",
        "    windows.append(window_features)\n",
        "    labels.append(label)\n",
        "  return np.array (windows), np.array(labels)"
      ],
      "metadata": {
        "id": "w_QLBqXILumm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_advanced_rnn_model(input_shape):\n",
        "  \"\"\"Enhanced RNN model with CNN-RNN hybrid architecture\"\"\"\n",
        "  model= Sequential([\n",
        "      #Convolutional layer for feature extraction\n",
        "      Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape,\n",
        "             kernel_regularizer=l2(0.001)),\n",
        "      MaxPooling1D(pool_size=2),\n",
        "      BatchNormalization(),\n",
        "\n",
        "\n",
        "  #Bidirectional RNN layers with increased complexity\n",
        "  Bidirectional(GRU(128, return_sequences=True,\n",
        "                    kernel_regularizer=l2(0.001))),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.4),\n",
        "\n",
        "  Bidirectional(LSTM(96, return_sequences=True,\n",
        "                     kernel_regularizer=l2(0.001))),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.4),\n",
        "\n",
        "  #Additional RNN layer\n",
        "  SimpleRNN(64, kernel_regularizer=l2(0.001)),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.3),\n",
        "\n",
        "  #Dense layers for classification with increased regularization\n",
        "  Dense(128, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.3),\n",
        "\n",
        "  Dense(64, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "  Dense(1, activation='sigmoid')\n",
        "\n",
        "  ])\n",
        "\n",
        "  #More aggresive optimizer configuration\n",
        "  optimizer=Adam(\n",
        "      learning_rate=0.0003,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999\n",
        "  )\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "E-9nbKc-RMb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(X, y, base_dir):\n",
        "  \"\"\"Comprehensive model training with cross-validation\"\"\"\n",
        "  #Use StandardScaler for better feature scaling\n",
        "  scaler=StandardScaler()\n",
        "  X_scaled=scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "\n",
        "  #Stratified K-Fold cross validation with more splits\n",
        "  kfold=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "  #Tracking metrics\n",
        "  cv_scores={\n",
        "      'accuracy':[], 'precision':[],\n",
        "      'recall':[], 'f1':[], 'auc':[]\n",
        "  }\n",
        "\n",
        "  #Model checkpoint directory\n",
        "  model_dir=os.path.join(base_dir, 'best_models')\n",
        "  os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(kfold.split(X_scaled, y), 1):\n",
        "    print(f\"\\n=== Fold {fold}===\")\n",
        "\n",
        "    X_train, X_val=X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val=y[train_idx], y[val_idx]\n",
        "\n",
        "    #Reset model for each fold\n",
        "    model=build_advanced_rnn_model((X.shape[1], X.shape[2]))\n",
        "\n",
        "    #Improved callbacks\n",
        "    callbacks=[\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=20,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.3,\n",
        "            patience=10,\n",
        "            min_lr=1e-6\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=os.path.join(model_dir, f'best_model_fold_{fold}.keras'),\n",
        "            monitor='val_recall',\n",
        "            mode='max',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    #Train model with more aggressive class weighting\n",
        "    history=model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        class_weight={0:1., 1: 3.}, #Stronger emphasis on event class\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    #Predictions and evaluation\n",
        "    predictions=model.predict(X_val).flatten()\n",
        "    binary_predictions=(predictions >0.5).astype(int)\n",
        "\n",
        "    #compute metriics\n",
        "    accuracy=accuracy_score(y_val, binary_predictions)\n",
        "    precision=precision_score(y_val, binary_predictions)\n",
        "    recall=recall_score(y_val, binary_predictions)\n",
        "    f1=f1_score(y_val, binary_predictions)\n",
        "    auc=roc_auc_score(y_val, predictions)\n",
        "\n",
        "    cv_scores['accuracy'].append(accuracy)\n",
        "    cv_scores['precision'].append(precision)\n",
        "    cv_scores['recall'].append(recall)\n",
        "    cv_scores['f1'].append(f1)\n",
        "    cv_scores['auc'].append(auc)\n",
        "\n",
        "    #Print detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, binary_predictions))\n",
        "\n",
        "    #Plot ROC curve\n",
        "    plot_roc_curve(y_val, predictions, base_dir, fold)\n",
        "    plot_training_history(history, base_dir)\n",
        "    plot_confusion_matrix(y_val, binary_predictions, base_dir)\n",
        "    plot_precision_recall_curve(y_val, predictions, base_dir)\n",
        "\n",
        "  #Print average cross-validation scores\n",
        "  print(\"\\n=== Cross-Validation Results ===\")\n",
        "  for metric, scores in cv_scores.items():\n",
        "    print(f\"{metric.capitalize()}: {np.mean(scores):.4f} (±{np.std(scores):.4f})\")\n",
        "\n",
        "  return model, cv_scores\n"
      ],
      "metadata": {
        "id": "xmiEJ-DBUnXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_true, y_scores, base_dir, fold):\n",
        "  \"\"\"Plot and save ROC curve\"\"\"\n",
        "  fpr, tpr, thresholds=roc_curve(y_true, y_scores)\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.plot(fpr, tpr, color='blue', label='ROC curve')\n",
        "  plt.plot([0,1], [0,1], color='red', linestyle='--', label='Random Classifier')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title(f'Receiver Operating Characteristic- Fold{fold}')\n",
        "  plt.legend(loc='lower right')\n",
        "\n",
        "  #Save plot\n",
        "  plt.savefig(os.path.join(base_dir, f'roc_curve_fold_{fold}.png'))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "V48WbBYnbtJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, base_dir):\n",
        "  \"\"\"Plot Training and validation metrics\"\"\"\n",
        "  plt.figure(figsize=(15, 10))\n",
        "\n",
        "  plt.subplot(2,2,1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title(\"Model Loss\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(2,2,2)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title(\"Model Accuracy\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, 'training_metric.png'))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "debzDBWTdXVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, base_dir):\n",
        "  cm=confusion_matrix(y_true, y_pred)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "              xticklabels=['No Event', 'Event'],\n",
        "              yticklabels=['No Event', 'Event'])\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted Label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, 'confusion_matrix.png'))\n",
        "  plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "L1BXjRZHdlNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_precision_recall_curve(y_true, y_scores, base_dir):\n",
        "  precision, recall, _=precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.plot(recall, precision, color='blue', label='Precision-Recall Curve')\n",
        "  plt.title('Precision-Recall curve')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, 'precision_recall_curve.png'))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "LnBC-vpUfVt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_seismic_data(real_data, synthetic_data, combined_data, base_dir):\n",
        "  plt.figure(figsize=(15, 10))\n",
        "\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.plot(real_data)\n",
        "  plt.title('Real Seismic Data')\n",
        "  plt.xlabel('Samples')\n",
        "  plt.ylabel('Amplitude')\n",
        "\n",
        "  plt.subplot(3,1,2)\n",
        "  plt.plot(synthetic_data)\n",
        "  plt.title('Synthetic Seismic Data')\n",
        "  plt.xlabel('Sample')\n",
        "  plt.ylabel('Amplitude')\n",
        "\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.plot(combined_data)\n",
        "  plt.title('Combined Seismic Data')\n",
        "  plt.xlabel('Sample')\n",
        "  plt.ylabel('Amplitude')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, \"seismic_data_comparison.png\"))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "5vGtx7hugJCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'window_size': 250,\n",
        "        'step': 25,\n",
        "        'sample_rate': 100,\n",
        "        'num_samples': 25000\n",
        "    }\n",
        "\n",
        "    # Create output directory\n",
        "    base_dir = create_output_directory()\n",
        "\n",
        "    # Fetch real earthquake data\n",
        "    origin_time = UTCDateTime(\"2015-08-11T16:22:15.200000\")\n",
        "    real_stream = fetch_iris_data(\n",
        "        network='YS', station='BAOP', location='',\n",
        "        channel='BHZ', origin_time=origin_time, duration=120\n",
        "    )\n",
        "\n",
        "    # Process real and synthetic data\n",
        "    if real_stream is not None:\n",
        "        filtered_real_stream, real_cft = process_stream(real_stream)\n",
        "        real_data = filtered_real_stream[0].data\n",
        "        sample_rate = real_stream[0].stats.sampling_rate\n",
        "    else:\n",
        "        # Fallback to default values if no real data\n",
        "        real_data = np.zeros(config['num_samples'])\n",
        "        sample_rate = config['sample_rate']\n",
        "        real_cft = None\n",
        "\n",
        "    # Generate synthetic data\n",
        "    synthetic_data, synthetic_events, synthetic_event_locations = generate_synthetic_data(\n",
        "        len(real_data),\n",
        "        sample_rate,\n",
        "        event_duration=2,\n",
        "        noise_level=0.1\n",
        "    )\n",
        "\n",
        "    # Combine datasets\n",
        "    combined_data = real_data + synthetic_data\n",
        "\n",
        "    # Create windows and labels\n",
        "    X, y = create_enhanced_windows(\n",
        "        combined_data,\n",
        "        config['window_size'],\n",
        "        config['step'],\n",
        "        synthetic_event_locations,\n",
        "        real_cft\n",
        "    )\n",
        "\n",
        "    # Train and evaluate model\n",
        "    trained_model, cv_scores = train_and_evaluate_model(X, y, base_dir)\n",
        "    plot_seismic_data(real_data, synthetic_data, combined_data, base_dir)\n",
        "\n",
        "    print(\"\\nModel training and evaluation complete. Check the output directory for visualizations.\")"
      ],
      "metadata": {
        "id": "4xi66A8AlpCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "TSjpEL5okayU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}