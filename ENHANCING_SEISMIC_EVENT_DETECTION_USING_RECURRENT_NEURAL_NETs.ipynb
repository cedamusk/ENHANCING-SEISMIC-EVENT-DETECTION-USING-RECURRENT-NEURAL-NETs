{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedamusk/ENHANCING-SEISMIC-EVENT-DETECTION-USING-RECURRENT-NEURAL-NETs/blob/main/ENHANCING_SEISMIC_EVENT_DETECTION_USING_RECURRENT_NEURAL_NETs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies\n",
        "##Purposes\n",
        "This code installs Python Libraries necessary for the project for machine learnin, signal processing and data visualization\n",
        "\n",
        "#Libraries\n",
        "1. Tensorflow\n",
        "An open source machine learning framework for creating and deploying deep learning mmodels\n",
        "\n",
        "#Obspy\n",
        "Python library designed for processing and analysing seismological data.\n",
        "\n",
        "It provides tools fro reading, writing and processing waveform data and supports seismic event analysis.\n",
        "\n",
        "#Matplotlib\n",
        "Matplotlib is a plotting library used for creating static, interactive and animated visualizations.\n",
        "\n",
        "It helps visualize seismic waveforms, and machine learning model outputs.\n",
        "\n",
        "#Scikit-learn\n",
        "Machin learning library that provides tools for classification and providing the metrics of scoring the model.\n",
        "\n",
        "It is also used in feature engineering.\n"
      ],
      "metadata": {
        "id": "3uhH669_Oste"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVKM8cxB_Zf4",
        "outputId": "37e992d6-3ff2-4e4d-be46-fcff3a66dcfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting obspy\n",
            "  Downloading obspy-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.11/dist-packages (from obspy) (1.13.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from obspy) (5.3.1)\n",
            "Collecting sqlalchemy<2 (from obspy)\n",
            "  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from obspy) (4.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2->obspy) (3.1.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading obspy-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sqlalchemy, obspy\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.38\n",
            "    Uninstalling SQLAlchemy-2.0.38:\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow obspy matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries and modules\n",
        "## Core libraries\n",
        "`numpy` and `matplotlib.pyplot`: For numerical operations and data visualization.\n",
        "`os`: For file and directory management.\n",
        "\n",
        "## Seismology tools\n",
        "`obspy`: Manages seismic data, including date/time handling (`UTCDateTime`), data streams (`Stream`, `Trace`), and fetching data from online servers (`Client`). It also includes signal processing methods like `classic_sta_lta` for detecting seismic events and `bandpass` for filtering.\n",
        "\n",
        "## Machine Learning Tools\n",
        "`sklearn`: Provides tools for data splitting (`train_test_split`, `StratifiedKFold`), scaling (`MinMaxScaler`, `StandardScaler`), and performance evaluation metrics (precision, recall, F1, confusion matrix, ROC curve)\n",
        "\n",
        "##Deep Learning Tools\n",
        "`tensorflow.keras`: For creating and training RNN models. Key components include:\n",
        "\n",
        "*   **Model Building**: `Sequential`\n",
        "*   **RNN Layers**: `LSTM`, `GRU`, `SimpleRNN`, `Bidirectional`\n",
        "*   **CNN Layers**: `Conv1D`, `MaxPooling1D`\n",
        "*   **Regularization and Optimization**: `l2`, `Adam`\n",
        "*   **Training Callbacks**: `EarlyStopping`, `ReduceLROnPlateau`, `ModelCheckpoint`.\n",
        "\n",
        "##Visualization Tools\n",
        "`seaborn`: For advanced statistical plotting.\n",
        "`tensorflow`: Core library for building and deploying machine learning models.\n",
        "\n"
      ],
      "metadata": {
        "id": "QVK8_McHPNoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from obspy import UTCDateTime, Stream, Trace\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy.signal.trigger import classic_sta_lta\n",
        "from obspy.signal.filter import bandpass\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (LSTM, Dense, Dropout, BatchNormalization,\n",
        "                                     Bidirectional, SimpleRNN, GRU, Conv1D, MaxPooling1D)\n",
        "from tensorflow.keras.callbacks import(\n",
        "    EarlyStopping, ReduceLROnPlateau,\n",
        "    ModelCheckpoint\n",
        ")\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.metrics import (\n",
        "    precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix,\n",
        "    accuracy_score, roc_auc_score, roc_curve,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "4S1Tf_eA_cks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create output directory\n",
        "The function `create_output_directory()` creates a directory named `seismic_detection_outputs` for saving plots, models or any other project related files.\n",
        "\n",
        "##Define directory name\n",
        "`base_dir='seismic_detection_outputs'`sets the directory name\n",
        "\n",
        "##Create directory\n",
        "`os.makedirs(base_dir, exist_ok=True)`creates the directory if it doesn't already exist. The `exist_ok=True` parameter prevents errors if the directory already exists.\n",
        "\n",
        "##Return Directory Path\n",
        "The function returns the directory path `base_dir` or later use in saving files."
      ],
      "metadata": {
        "id": "JgiY-HvBSqvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_output_directory():\n",
        "  \"\"\"Create a directory for saving plots and models\"\"\"\n",
        "  base_dir='seismic_detection_outputs'\n",
        "  os.makedirs(base_dir, exist_ok=True)\n",
        "  return base_dir"
      ],
      "metadata": {
        "id": "6Q38pRGADqNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch IRIS data\n",
        "`network`: Seismic network code\n",
        "\n",
        "`station`: Seismic station code\n",
        "\n",
        "`location`: Station location identifier\n",
        "\n",
        "`channel`:Channel code for seismic data\n",
        "\n",
        "`origin_time`: Start time for fetching, in UTC format\n",
        "\n",
        "`duration`: Length of time (in seconds) to fetch data after `origin_time`.\n",
        "\n",
        "##Function workflow\n",
        "1. **Initialize Client**: `client=Client(\"IRIS\")` sets up a connection to the IRIS database.\n",
        "\n",
        "2. **Define time range**: `start_time=origin_time` sets the starting point. `end_time=origin_time+duration` calculates the ending point.\n",
        "\n",
        "3. **Fetch Seismic data**: `stream=client.get_waveforms()` fetches seismic waveform data based on the provided parameters.\n",
        "\n",
        "4. **Handle errors**: If the data fetching process fails, the function prints an error message and returns `None`.\n",
        "\n",
        "##Return Value\n",
        "Returns the `stream` object containing the seismic waveform data if successful or `None` if an error occurs.\n"
      ],
      "metadata": {
        "id": "TGL4GKjsTzce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_iris_data(network, station, location, channel, origin_time, duration=120):\n",
        "  \"\"\"Fetch Seismic data from the IRIS database\"\"\"\n",
        "  try:\n",
        "    client=Client(\"IRIS\")\n",
        "    start_time=origin_time\n",
        "    end_time=origin_time+duration\n",
        "\n",
        "    stream=client.get_waveforms(network, station, location, channel, start_time, end_time)\n",
        "    return stream\n",
        "  except Exception as e:\n",
        "    print(f\"Error fetching data from IRIS:{str(e)}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "Zl83CUf0E0G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Synthetic Data\n",
        "The function creates synthetic seismic waveform data, including background noise and multiple seismic-like events.\n",
        "\n",
        "##Function parameters\n",
        "`num_samples`: Total number of samples in the synthetic signal.\n",
        "\n",
        "`sample_rate`: Number of samples per second.\n",
        "\n",
        "`event_duration`: Length of each seismic event in seconds.\n",
        "\n",
        "`noise_level`: Standard deviation of the background noise.\n",
        "\n",
        "`num_events`: Maximum number of events to generate (default is 3).\n",
        "\n",
        "##Function workflow\n",
        "1. **Initialize variables**:\n",
        "\n",
        "`time`: Array representing the time points of the synthetic signal.\n",
        "\n",
        "`background`: Random nise with mean 0 and standard deviation `noise_level`.\n",
        "\n",
        "`events`: Zero-initialized array for storing events.\n",
        "\n",
        "`event_locations`: List to store start and end indices of generated events.\n",
        "\n",
        "\n",
        "2. **Generate seismic events**:\n",
        "Loop through a random number of events (between 1 and `num_events`).\n",
        "\n",
        "**Event location**:Randomly select an event start index between 1/8 and 7/8 of the signal to avoid edge effects.\n",
        "\n",
        "Calculate the event's and index based on `event_duration`.\n",
        "\n",
        "**Event Characetristics**: Randomly generate frequencies (`freq1`, `freq2`), amplitudes (`amp1`, `amp2`) and decay rates (`decay1`, `decay2`) for two sine wave components.\n",
        "\n",
        "Create the event as a decaying sinusoidal combination to simulate realistic seismic waveforms.\n",
        "\n",
        "**Insert Event**: Add the generated event to the corresponding range in the `events` array.\n",
        "\n",
        "3.  **Combine Background and Events**: add `background` and `events` to create the final synthetic data.\n",
        "\n",
        "##Return Values\n",
        "`data`: Combined synthetic waveform (background + events)\n",
        "`events`: Isolated events without noise.\n",
        "`event_locations`: List of tuples indicating event start and end indices.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__xggnk2WxXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_synthetic_data(num_samples, sample_rate, event_duration, noise_level, num_events=3):\n",
        "  \"\"\"Generate synthetic data with multiple events\"\"\"\n",
        "  time=np.arange(num_samples)/ sample_rate\n",
        "  background=np.random.normal(0, noise_level, num_samples)\n",
        "  events=np.zeros(num_samples)\n",
        "  event_locations=[]\n",
        "\n",
        "  for _ in range(np.random.randint(1, num_events+1)):\n",
        "    event_start=np.random.randint(num_samples // 8, num_samples*7//8)\n",
        "    event_end=event_start + int(event_duration*sample_rate)\n",
        "    event_locations.append((event_start, event_end))\n",
        "\n",
        "    #Create more realistic events with multiple frequency components\n",
        "    freq1=np.random.uniform(3,8)\n",
        "    freq2=np.random.uniform(10, 20)\n",
        "    amp1=np.random.uniform(1.0, 2.0)\n",
        "    amp2=np.random.uniform(0.5, 1.5)\n",
        "    decay1=np.random.uniform(0.1, 0.3)\n",
        "    decay2=np.random.uniform(0.2, 0.4)\n",
        "\n",
        "    event_time=time[event_start:event_end]-time[event_start]\n",
        "    event=(\n",
        "        amp1*np.sin(2*np.pi*freq1*event_time)*np.exp(-event_time/decay1)+\n",
        "        amp2*np.sin(2*np.pi*freq2*event_time)*np.exp(-event_time/decay2)\n",
        "    )\n",
        "    events[event_start:event_end]=event\n",
        "  data=background+events\n",
        "  return data, events, event_locations"
      ],
      "metadata": {
        "id": "TCAo4L3XFqiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Stream\n",
        "This function performs advanced preprocessing on a seismic data stream to enhance signal quality and detect seismix events using the STA/LTA method.\n",
        "\n",
        "## Function parameters\n",
        "`stream`: The seismic data stream to process.\n",
        "`freqmin`: Minimum frequency for bandpass filtering (default is 0.5 Hz).\n",
        "`freqmax`: Maximum frequency fo bandpass filtering (default is 20 Hz).\n",
        "\n",
        "## Processing Workflow\n",
        "1. **Create a copy**: `processed_stream=stream.copy()` avoids modifying the original data.\n",
        "\n",
        "2. **Preprocessing Steps**:\n",
        "\n",
        "`detrend('linear')`:Removes linear trends from the data\n",
        "\n",
        "`taper(max_percentage=0.1)`: Applies a 10% taper to reduce edge effects.\n",
        "\n",
        "`filter ('bandpass'...)`: Applies a 6th-order zero-phase bandpass filter to isolate relevant seismic frequencies.\n",
        "\n",
        "3. **STA/LTA Trigger Calculation**:\n",
        "\n",
        "Extract the first trace: `trace=processed_stream[0]`\n",
        "\n",
        "Apply the STA/LTA algorithm `classic_sta_lta(trace.data, int(0.5*trace.stats.sampling_rate), int(10*trace.stats.sampling_rate))` computed the characteristic function `cft`.\n",
        "\n",
        "The short-term average (STA) window is 0.5 seconds.\n",
        "\n",
        "The long-term average (LTA) window is 10 seconds.\n",
        "\n",
        "##Return values\n",
        "`processed_stream`: The preprocessed seismic stream.\n",
        "`cft`: The STA/LTA characteristic function for event detection."
      ],
      "metadata": {
        "id": "rOyqPT6QgA5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_stream(stream, freqmin=0.5, freqmax=20):\n",
        "  \"\"\"Enhanced stream processing with more robust filtering\"\"\"\n",
        "  processed_stream=stream.copy()\n",
        "  processed_stream.detrend('linear')\n",
        "  processed_stream.taper(max_percentage=0.1)\n",
        "  processed_stream.filter('bandpass', freqmin=freqmin, freqmax=freqmax,\n",
        "                          corners=6, zerophase=True)\n",
        "\n",
        "  #Add STA/LTA trigger for additional event detection\n",
        "  trace=processed_stream[0]\n",
        "  cft=classic_sta_lta(trace.data, int(0.5*trace.stats.sampling_rate),\n",
        "                      int(10*trace.stats.sampling_rate))\n",
        "  return processed_stream, cft"
      ],
      "metadata": {
        "id": "BPsZSOXbKrnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced windows\n",
        "The function generates labelled data windows with advanced feature engineering for seismic event prediction. It uses synthetic event location, STA/LTA triggers and statistical anomaly detection for labelling.\n",
        "\n",
        "##Function Parameters\n",
        "`data`: The input seismic data array.\n",
        "\n",
        "`window_size`: Number of samples per window.\n",
        "\n",
        "`step`: Step size for moving the window along the data.\n",
        "\n",
        "`event_locations`: List of event start and end indices from synthetic data (optional).\n",
        "\n",
        "`cft`: STA/LTA characteristic function for event detection (optional)\n",
        "\n",
        "\n",
        "##Inner Function\n",
        "`detect_event_by_trigger(window_start, window_end, cft)`\n",
        "Checks if the maximum STA/LTA value within a window exceeds a threshold (3.0).\n",
        "\n",
        "Returns `True` if an event is detected, otherwise `False`.\n",
        "\n",
        "##Processing Workflow\n",
        "\n",
        "1. **Initialize Lists**:\n",
        "`windows=[]`: To store windowed features.\n",
        "\n",
        "`labels=[]`: To store corresponding labels.\n",
        "\n",
        "2. **Loop through data**:\n",
        "\n",
        "Sliding window from start to end of `data` with the specified `step`.\n",
        "\n",
        "Extract a data `window` of size `window_size`.\n",
        "\n",
        "3. **Feature Engineering**:\n",
        "\n",
        "**Time Domain Features**:\n",
        "*  Raw signal (`window`).\n",
        "*  Absolute amplitude (`np.abs(window)`)\n",
        "*  First and second deravitives (`np.gradient(window)`), `np.gradient(np.gradient(window))`)\n",
        "*  Log-transformed absolute amplitude (`np.log1p(np.abs(window))`).\n",
        "\n",
        "**Frequency Domain features**\n",
        "Compute the real part of the FFT (`np.fft.fft(window)`), keeping half of the spectrum.\n",
        "\n",
        "Pad the FFt result to match `window_size`.\n",
        "\n",
        "Stack all features into `window_features`.\n",
        "\n",
        "4. **Labelling logic**:\n",
        "Initialize `label=0` (no event detected).\n",
        "\n",
        "**Synthetic event check**\n",
        "Check if window overlaps with any event in `event_locations`.\n",
        "\n",
        "**STA/LTA Trigger check**:\n",
        "If no synthetic event is found, use STA/LTA detection.\n",
        "\n",
        "**Anomaly Detection (Fallback)**:\n",
        "If still no event, compare the window's mean absolute value to the data's standard deviation.\n",
        "\n",
        "Assign `label=1`if the mean exceeds 2.5 times the standard deviation,\n",
        "\n",
        "5. **Store results**\n",
        "\n",
        "Append `window_features` to `windows`.\n",
        "Append `label` to `labels`.\n",
        "\n",
        "##Return Values\n",
        "\n",
        "`windows`: A 2D array of windowed features for model input.\n",
        "`labels`: Corresponding labels (0 for no event, 1 for event).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fip128sgir18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_enhanced_windows(data, window_size, step, event_locations=None, cft=None):\n",
        "  \"\"\"Create window with advanced feature engineering and labelling\"\"\"\n",
        "  windows=[]\n",
        "  labels=[]\n",
        "\n",
        "  #add STA/LTA trigger-based event detection if available\n",
        "  def detect_event_by_trigger(window_start, window_end, cft=None):\n",
        "    if cft is not None:\n",
        "      window_cft=cft[window_start:window_end]\n",
        "      return np.max(window_cft)>3.0 #Adjust threshold as needed\n",
        "    return False\n",
        "\n",
        "  for i in range(0, len(data)-window_size+1, step):\n",
        "    window=data[i:i+window_size]\n",
        "    freq_features=np.fft.fft(window)[:window_size//2].real\n",
        "    freq_features_padded=np.pad(freq_features, (0, window_size-len(freq_features)), 'constant')\n",
        "\n",
        "    #Advanced feature engineering\n",
        "    window_features=np.column_stack([\n",
        "        window, #Raw signal\n",
        "        np.abs(window), #Absolute amplitude\n",
        "        np.gradient(window), #First deravitive\n",
        "        np.gradient(np.gradient(window)), #Second derivative\n",
        "        np.log1p(np.abs(window)), #Log of absolute amplitude\n",
        "        freq_features_padded, #Padded frequency domain features\n",
        "    ])\n",
        "\n",
        "    #Labelling logic with multiple detection methods\n",
        "    label=0\n",
        "    if event_locations:\n",
        "      #Check if window contains an event from synthetic data\n",
        "      for start, end in event_locations:\n",
        "        if (i<=end and i +window_size >= start):\n",
        "          label=1\n",
        "          break\n",
        "\n",
        "    #Additional event detection using STA/LTA\n",
        "    if label==0 and detect_event_by_trigger(i, i +window_size, cft):\n",
        "      label=1\n",
        "\n",
        "    #Fall back: statistical anomaly detection\n",
        "    if label==0:\n",
        "      window_mean=np.abs(window).mean()\n",
        "      data_std=np.abs(data).std()\n",
        "      label=1 if window_mean> data_std *2.5 else 0\n",
        "\n",
        "    windows.append(window_features)\n",
        "    labels.append(label)\n",
        "  return np.array (windows), np.array(labels)"
      ],
      "metadata": {
        "id": "w_QLBqXILumm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build advanced RNN model\n",
        "The function defines a deep learning model combining Convolutional Neural Netwroks (CNNs) and Recurrent Neural Networks (RNNs) for seismic event prediction. This hybrid architecture leverages both spatial and temporal feature extraction techniques.\n",
        "\n",
        "##Function Parameter\n",
        "`input_shape`: Shape of the input data, typically `(window_size, num_features)`.\n",
        "\n",
        "##Model Architecture:\n",
        "\n",
        "1. **Convolutional Layer for Feature Extraction**:\n",
        "`Conv1D(64, kernel_size=5, activation='relu')`: Extracts spatial features with 64 filter and a kernel size of 5.\n",
        "\n",
        "`MaxPooling1D(pool_size=2)`: Reduces the dimensionality be half.\n",
        "\n",
        "`BatchNormaliation()`: Normalizes the activations to stabilie and accelerate trainig.\n",
        "\n",
        "2. **Recurrent Layers for Temporal Dependenices**:\n",
        "`Bidirectional(GRU(128, return_sequences=True))`: Captures long-term dependencies in both forward and backward directions.\n",
        "`return_sequences=True` enables stacking further RNN layers.\n",
        "\n",
        "`BatchNormalization()` and `Dropout(0.4)`: Improves training stability and prevents overfitting.\n",
        "\n",
        "`Bidirectional(LSTM(96, return_sequences=True))`: Similar to the GRU layer but with increased model complexity.\n",
        "\n",
        "`SimpleRNN(64)`: Adds a final RNN layer to extract additional sequential patterns.\n",
        "\n",
        "3. **Dense Layers for Classification**:\n",
        "`Dense(128, activation='relu)`: fully connected layer with ReLU activation and L2 regularization\n",
        "\n",
        "`BatchNormalization()` & `Dropout(0.3)`: Stabilizes training and reduces overfitting.\n",
        "\n",
        "`Dense(64, activation='relu)`: Second fully connected layer for deeper feature integration.\n",
        "\n",
        "`Dense(1, activation='sigmoid')`: Final output layer for binary classification (seismic event or no event).\n",
        "\n",
        "##Optimizer and Compilation\n",
        "**Optimizer**: `Adam(learning_rate=0.0003, beta_1=0.9, beta_2=0.999)` is configured for more aggressive optimization with a low learning rate.\n",
        "\n",
        "**Loss function**: `binary_crossentropy` is used for binary classification task.\n",
        "\n",
        "**Metrics**: `accuracy`,`Precision()` and `Recall()` monitor the model's performance comprehensively.\n",
        "\n",
        "##Return Value:\n",
        "The compiled model is returned, rady for training and evaluation\n"
      ],
      "metadata": {
        "id": "vjXDuHUQsTcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_advanced_rnn_model(input_shape):\n",
        "  \"\"\"Enhanced RNN model with CNN-RNN hybrid architecture\"\"\"\n",
        "  model= Sequential([\n",
        "      #Convolutional layer for feature extraction\n",
        "      Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape,\n",
        "             kernel_regularizer=l2(0.001)),\n",
        "      MaxPooling1D(pool_size=2),\n",
        "      BatchNormalization(),\n",
        "\n",
        "\n",
        "  #Bidirectional RNN layers with increased complexity\n",
        "  Bidirectional(GRU(128, return_sequences=True,\n",
        "                    kernel_regularizer=l2(0.001))),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.4),\n",
        "\n",
        "  Bidirectional(LSTM(96, return_sequences=True,\n",
        "                     kernel_regularizer=l2(0.001))),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.4),\n",
        "\n",
        "  #Additional RNN layer\n",
        "  SimpleRNN(64, kernel_regularizer=l2(0.001)),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.3),\n",
        "\n",
        "  #Dense layers for classification with increased regularization\n",
        "  Dense(128, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "  BatchNormalization(),\n",
        "  Dropout(0.3),\n",
        "\n",
        "  Dense(64, activation='relu', kernel_regularizer=l2(0.002)),\n",
        "  Dense(1, activation='sigmoid')\n",
        "\n",
        "  ])\n",
        "\n",
        "  #More aggresive optimizer configuration\n",
        "  optimizer=Adam(\n",
        "      learning_rate=0.0003,\n",
        "      beta_1=0.9,\n",
        "      beta_2=0.999\n",
        "  )\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "E-9nbKc-RMb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate the model\n",
        "This function trains and evaluates the RNN model using cross-validation, with various enhancements for model training, evaluation and performance tracking\n",
        "\n",
        "##Function Parameters\n",
        "`X`: Input feature for training, shaped as `(num_samples, num_timesteps, num_features)`.\n",
        "\n",
        "`y`: Labels for the samples, either 0 (no event) or 1 (event).\n",
        "\n",
        "`base_dir`: directory to save model checkpoints, plots, and evaluation results.\n",
        "\n",
        "##Function Workflow:\n",
        "1. **Feature Scaling**:\n",
        "The function uses `StandardScaler() to scale the input features (`X`) to have a mean of 0 and a standard deviation of 1 for improved model performance.\n",
        "\n",
        "`X_scaled=scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)`reshapes and scales the input data accordingly.\n",
        "\n",
        "2. **Cross-Validation setup**:\n",
        "**Stratified K-Fold Cross Validation**: `StratifiedKFold(n_splits=10)` ensures that each fold has a balanced distribution of classes (seismic event and no event).\n",
        "\n",
        "The cross-validation splits the data into 10 folds.\n",
        "\n",
        "3. **Model training and evaluation**:For each fold:\n",
        "\n",
        "**Training and Validation split**: The data is divided into training (`X_train, y_train`) using the indices provided by `StratifiedKFold`.\n",
        "\n",
        "**Model Initialization**:\n",
        "The RNN model is re-initialized for each fold to avoid training from previous fold weights.\n",
        "\n",
        "**Callbacks**:\n",
        "Several callbacks are used for better training management:\n",
        "*  **EarlyStopping: Stops training if the validation loss doesn't improve after 20 epochs, restoring the best weights.\n",
        "\n",
        "*  **ReduceLROnPLateau**: Reduces the learning rate by a factor of 0.3 if the validation loss doesn't improve for 10 epochs.\n",
        "\n",
        "*  **ModelCheckpoint**: Svaes the best model based on the validation recall score.\n",
        "\n",
        "**Model training**:\n",
        "The model is trained for a maximum of 150 epochs with a batch size of 32. The `class_weight={0:1., 1:3.}` argument assigns more weight to the event class (class`1`), compensating for class imbalance.\n",
        "\n",
        "**Predictions and Metrics**:\n",
        "The model's predictions are made on the validation set, and binary classification is performed by thresholding the predicted probabilities at 0.5.\n",
        "\n",
        "Metrics calculated include:\n",
        "1.  Accuracy\n",
        "2.  Precision\n",
        "3.  Recall\n",
        "4. F1 score\n",
        "5. AUC(Area Under the Curve)\n",
        "\n",
        "**Plots**:\n",
        "Several plots are generated for performance visualization:\n",
        "*  **ROC Curve**(`plot_roc_curve`)\n",
        "*  **Training history**(`plot_training_history`)\n",
        "*  **Confusion Matrix**(`plot_confusion_matrix`)\n",
        "*  **Precision-Recall Curve**(`plot_precision-recall_curve`)\n",
        "\n",
        "4. **Cross-validation results**:\n",
        "After completing all the folds, the function prints the average cross-validation score for each metric (accuracy, precision, recall, F1 Score and AUC) with their standard deviations.\n",
        "\n",
        "##Return values\n",
        "`model`: The trained model from the final fols, saved after training.\n",
        "`cv_scores`: A dictionary containing cross-validation scores for eac metric accuracy, precision, recall, F1 Score and AUC\n",
        "\n",
        "##Key enhancements\n",
        "**Cross-Validation**: Using Stratified K-Fold ensures balabced training and testing sets.\n",
        "\n",
        "**Regularization**: includes L2 regularization, dropout, and batch normalization to improve generalization and avoid overfitting.\n",
        "\n",
        "**Class weighing**: More emphasis on detecting seismic events by applying stronger weight to class `1`.\n",
        "\n",
        "**Comprehensive Evaluation**: Tracks a variety of performance metrics, generates detailed reports and visualizes results through multiple plots.\n",
        "\n"
      ],
      "metadata": {
        "id": "OC29plo0yZzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_model(X, y, base_dir):\n",
        "  \"\"\"Comprehensive model training with cross-validation\"\"\"\n",
        "  #Use StandardScaler for better feature scaling\n",
        "  scaler=StandardScaler()\n",
        "  X_scaled=scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)\n",
        "\n",
        "  #Stratified K-Fold cross validation with more splits\n",
        "  kfold=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "  #Tracking metrics\n",
        "  cv_scores={\n",
        "      'accuracy':[], 'precision':[],\n",
        "      'recall':[], 'f1':[], 'auc':[]\n",
        "  }\n",
        "\n",
        "  #Model checkpoint directory\n",
        "  model_dir=os.path.join(base_dir, 'best_models')\n",
        "  os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(kfold.split(X_scaled, y), 1):\n",
        "    print(f\"\\n=== Fold {fold}===\")\n",
        "\n",
        "    X_train, X_val=X_scaled[train_idx], X_scaled[val_idx]\n",
        "    y_train, y_val=y[train_idx], y[val_idx]\n",
        "\n",
        "    #Reset model for each fold\n",
        "    model=build_advanced_rnn_model((X.shape[1], X.shape[2]))\n",
        "\n",
        "    #Improved callbacks\n",
        "    callbacks=[\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=20,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.3,\n",
        "            patience=10,\n",
        "            min_lr=1e-6\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=os.path.join(model_dir, f'best_model_fold_{fold}.keras'),\n",
        "            monitor='val_recall',\n",
        "            mode='max',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    #Train model with more aggressive class weighting\n",
        "    history=model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=150,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        class_weight={0:1., 1: 3.}, #Stronger emphasis on event class\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    #Predictions and evaluation\n",
        "    predictions=model.predict(X_val).flatten()\n",
        "    binary_predictions=(predictions >0.5).astype(int)\n",
        "\n",
        "    #compute metriics\n",
        "    accuracy=accuracy_score(y_val, binary_predictions)\n",
        "    precision=precision_score(y_val, binary_predictions)\n",
        "    recall=recall_score(y_val, binary_predictions)\n",
        "    f1=f1_score(y_val, binary_predictions)\n",
        "    auc=roc_auc_score(y_val, predictions)\n",
        "\n",
        "    cv_scores['accuracy'].append(accuracy)\n",
        "    cv_scores['precision'].append(precision)\n",
        "    cv_scores['recall'].append(recall)\n",
        "    cv_scores['f1'].append(f1)\n",
        "    cv_scores['auc'].append(auc)\n",
        "\n",
        "    #Print detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, binary_predictions))\n",
        "\n",
        "    #Plot ROC curve\n",
        "    plot_roc_curve(y_val, predictions, base_dir, fold)\n",
        "    plot_training_history(history, base_dir)\n",
        "    plot_confusion_matrix(y_val, binary_predictions, base_dir)\n",
        "    plot_precision_recall_curve(y_val, predictions, base_dir)\n",
        "\n",
        "  #Print average cross-validation scores\n",
        "  print(\"\\n=== Cross-Validation Results ===\")\n",
        "  for metric, scores in cv_scores.items():\n",
        "    print(f\"{metric.capitalize()}: {np.mean(scores):.4f} (±{np.std(scores):.4f})\")\n",
        "\n",
        "  return model, cv_scores\n"
      ],
      "metadata": {
        "id": "xmiEJ-DBUnXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Receiver Operating Characteristic\n",
        "The ROC curve visualizes the trade-off between the true positive rate and the false positive rate at different thresholds, helping evaluate the classifier's performance. A curve closer to the top-left corner indicates better performance.\n",
        "\n",
        "## Function definition and inputs\n",
        "`y_true`: Actual labels (ground truth).\n",
        "`y_scores`: Predicted probabilities from the model.\n",
        "`base_dir`: Directory where the ROC curve image will be saved.\n",
        "`fold`: The current fold number in cross-validation.\n",
        "\n",
        "##Compute ROC metrics\n",
        "`fpr, tpr, thresholds=roc_curve(y_true, y_scores):`\n",
        "\n",
        "`fpr`: False Positive Rate.\n",
        "`tpr`: True Positive Rate.\n",
        "`thresholds`: Decision thresholds used to compute `fpr` and `tpr`.\n",
        "\n",
        "##Plot the ROC Curve\n",
        "\n",
        "\n",
        "*   Sets the figure size to 8*6 inches\n",
        "*   Plots the ROC curve using `fpr` and `tpr`.\n",
        "*   Adds a diagonal red dashed line representing a random classifier (baseline).\n",
        "*   Adjusts the axis limits for better visualization.\n",
        "*   Labels the axes and titles the plot.\n",
        "*   Adds a legend to distinguish the model from the baseline.\n",
        "\n",
        "##Save the Plot\n",
        "*  Constructs the file path using `os.path.join`.\n",
        "*  Saves the plot as `roc_curve_fold_{fold}.png` in the specified directory.\n",
        "*  Closes the plot to free up memory and avoid overlapping features.\n",
        "\n"
      ],
      "metadata": {
        "id": "9oP-n-xRmauC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_curve(y_true, y_scores, base_dir, fold):\n",
        "  \"\"\"Plot and save ROC curve\"\"\"\n",
        "  fpr, tpr, thresholds=roc_curve(y_true, y_scores)\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.plot(fpr, tpr, color='blue', label='ROC curve')\n",
        "  plt.plot([0,1], [0,1], color='red', linestyle='--', label='Random Classifier')\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title(f'Receiver Operating Characteristic- Fold{fold}')\n",
        "  plt.legend(loc='lower right')\n",
        "\n",
        "  #Save plot\n",
        "  plt.savefig(os.path.join(base_dir, f'roc_curve_fold_{fold}.png'))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "V48WbBYnbtJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot training history\n",
        "This function visualizes the training and validation performance metrics from the model's training from the model's training history and saves the generated plots as an image file.\n",
        "\n",
        "##Function Definition and inputs\n",
        "`history`: Training history object returned by `model.fit()`.\n",
        "`base_dir`: Directory where the training metrics plot will be saved.\n",
        "\n",
        "##Figure Initialization\n",
        "Sets the figure size to `15*10` inches.\n",
        "\n",
        "##Subplot 1: Model Loss\n",
        "Plots training hand validation loss values from `history.history['loss']` and `history.history['val_loss']`.\n",
        "\n",
        "Labels the axis as \"Epoch\" and \"Loss\".\n",
        "\n",
        "Adds a title \"Model Loss\" and a legend to differentiate between training and validation loss.\n",
        "\n",
        "##Subplot 2: Model Accuracy\n",
        "Plots training and validation accuravy from `history.history['accuracy']` and `history.history['val_accuracy']`.\n",
        "\n",
        "Labels the axes as 'Epoch' and \"Accuracy\".\n",
        "\n",
        "Adds a title \"Model Accuracy\" and a legend to sitinguish between training and validation accurcay.\n",
        "\n",
        "##Layout adjustment and saving\n",
        "Uses `plt.tight_layout()` to optimize subplot spacing.\n",
        "\n",
        "Saves the figure as `training_metric.png` in the specifies `base_dir`.\n",
        "\n",
        "Closes the figure to prevent memory issues or overlapping plots.\n"
      ],
      "metadata": {
        "id": "0A9GLScMqTYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history, base_dir):\n",
        "  \"\"\"Plot Training and validation metrics\"\"\"\n",
        "  plt.figure(figsize=(15, 10))\n",
        "\n",
        "  plt.subplot(2,2,1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title(\"Model Loss\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('loss')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(2,2,2)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title(\"Model Accuracy\")\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, 'training_metric.png'))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "debzDBWTdXVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Confusion Matrix\n",
        "The function generates a heatmap of the confusion matrix from predicted and true labels,providing insight into the model's classification performance. The resulting plot is saved as an image file.\n",
        "\n",
        "##Function definition and inputs\n",
        "`y_true`: Actual labels from the validation dataset.\n",
        "\n",
        "`y_pred`: Predicted labels generated by the model.\n",
        "\n",
        "`base_dir`: Directory where the confusion matrix plot will be saved.\n",
        "\n",
        "##Compute Confusion Matrix\n",
        "Calss `confusion_matrix(y_true, y_pred)` to calculate the confusion matrix, which shows the number of correct and incorrect predictions for each class.\n",
        "\n",
        "##Create Heatmap plot\n",
        "Initializes a plot of size 8*6 inches.\n",
        "\n",
        "Uses `sns.heatmap()` to create a heatmap visualization.\n",
        "\n",
        "`cm`: confusion matrix data.\n",
        "\n",
        "`annot=True`: Displays numeric values on the heatmap.\n",
        "\n",
        "`fmt=d` formats annotations as integers.\n",
        "\n",
        "`cmap=Blues`: Sets the heatmap color theme.\n",
        "\n",
        "`xticklabels` and `yticklabels`: Labels for predicted and true classes.\n",
        "\n",
        "##Add titles and Labels\n",
        "Sets the plot title to \"Confusion Matrix\".\n",
        "\n",
        "Labels the x-axis as \"Predicted Label.\"\n",
        "\n",
        "Labels the y-axis as \"True Label.\"\n",
        "\n",
        "##Save the plot\n",
        "Adjusts layout with `plt.tight_layout()` to avoid clipping.\n",
        "\n",
        "Saves the figure as `confusion_matrix.png` in `base_dir`.\n",
        "\n",
        "Closes the figure to prevent overlapping or memory issues.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vAdbzTo3yIvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, base_dir):\n",
        "  cm=confusion_matrix(y_true, y_pred)\n",
        "  plt.figure(figsize=(8,6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "              xticklabels=['No Event', 'Event'],\n",
        "              yticklabels=['No Event', 'Event'])\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.xlabel('Predicted Label')\n",
        "  plt.ylabel('True label')\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, 'confusion_matrix.png'))\n",
        "  plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "L1BXjRZHdlNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Precision Recall Curve\n",
        "The function generates a Precision-Recall Curve which is a graphical representation of the tradeoff between precision and recall for different classification thresholds. It saves the plot as an image file in the specified directory.\n",
        "\n",
        "##Parameters\n",
        "`y_true`: True binary labels of the dataset(0, 1), indicating whether a data point belongs to the positive case.\n",
        "\n",
        "`y_scores`: Predicated probabilities or scores are assigned to the positive class by the classifier. These scores are used to compute precision adn recall at various thresholds.\n",
        "\n",
        "`base_dir`: Directory path where the generated plot will be saved.\n",
        "\n",
        "##Functionality\n",
        "1. **Computing Precision and Recall**\n",
        "The function uses `precision_recall_curve` from `sklearn.metrics` to compute precision and recall values over a range of thresholds.\n",
        "\n",
        "The function also outputs thresholds\n",
        "\n",
        "2. **Plotting the curve**\n",
        "A new figure is created with a size of `8*6` inches for clear visualization.\n",
        "\n",
        "Precision is plotted on the y-axis and recall is plotted on the x-axis. A blue line represents the curve.\n",
        "\n",
        "Labels and a legend are added to make the plot informative.\n",
        "\n",
        "3. **Adjusting the Layout**\n",
        "The `plt.tight_layout()` method ensures that the elements of the plot are arranged neatly without overlap.\n",
        "\n",
        "4. **Saving the Plot**\n",
        "The plot is saved as an image named `precision_recall_curve.png` in the specified `base_dir` using `os.path.join` for robust path handling.\n",
        "\n",
        "5. **Closing the plot**\n",
        "`plt.close()` is called to release resources and avoid displaying of the figure."
      ],
      "metadata": {
        "id": "4_Y1NwlEy4dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_precision_recall_curve(y_true, y_scores, base_dir):\n",
        "  precision, recall, _=precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.plot(recall, precision, color='blue', label='Precision-Recall Curve')\n",
        "  plt.title('Precision-Recall curve')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, 'precision_recall_curve.png'))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "LnBC-vpUfVt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Seismic Data\n",
        "The function is designed to visualize and compare seismi data from three diffeerent sources, the real seismic data, synthetic data and a combination of both. It generates a side-by-side comparison of these data sources in the form of a plot and saves the pot as an image file\n",
        "\n",
        "##Parameters\n",
        "`real_data`: The actual seismic data from Indonesian station.\n",
        "\n",
        "`synthetic_data`: Simulated seismic data generated through modelling or computational techniques.\n",
        "\n",
        "`combined_data`: Data resulting from the combination of real and synthetic data, often used to analyze correlation or validate models.\n",
        "\n",
        "`base_dir`: The directory path where the plot image will be saved.\n",
        "\n",
        "##Functionality\n",
        "1. **Figure Setup**\n",
        "The function creates a `matplotlib` figure with a size of `15*10` inches, providing ample space for three subplors stacked vertically.\n",
        "\n",
        "2. **Subplots for Visualization**\n",
        "First Subplot: Plots the `real_data` as \"Real Seismic Data\" with x-axis for samples and y-axis for amplitude.\n",
        "\n",
        "Second Subplot: Plots the `synthetic_data` array. Labels the plot as \"Synthetic Seismic Data\" with the same axis labels.\n",
        "\n",
        "Third Subplot: Plots the `combined_data`/ Labels the plot as \"Combined Seismic Data\", maintaining the labels.\n",
        "\n",
        "3. **Layout Adjustments**\n",
        "The function uses `plt.tight_layout()` to ensure there is no overlapping of subplot titles or axes.\n",
        "\n",
        "4. **Saving the Plot**\n",
        "The plot is saved as `seismic_data_comparison.png` in the specified `base_dir` directory using the `os.path.join` method.\n",
        "\n",
        "5. **Closing the Plot**\n",
        "`plt.close()` ensures the figure is properly closed, freeing up memory resources and avoiding display in the environment.\n"
      ],
      "metadata": {
        "id": "ACW9ucH5v5MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_seismic_data(real_data, synthetic_data, combined_data, base_dir):\n",
        "  plt.figure(figsize=(15, 10))\n",
        "\n",
        "  plt.subplot(3,1,1)\n",
        "  plt.plot(real_data)\n",
        "  plt.title('Real Seismic Data')\n",
        "  plt.xlabel('Samples')\n",
        "  plt.ylabel('Amplitude')\n",
        "\n",
        "  plt.subplot(3,1,2)\n",
        "  plt.plot(synthetic_data)\n",
        "  plt.title('Synthetic Seismic Data')\n",
        "  plt.xlabel('Sample')\n",
        "  plt.ylabel('Amplitude')\n",
        "\n",
        "  plt.subplot(3,1,3)\n",
        "  plt.plot(combined_data)\n",
        "  plt.title('Combined Seismic Data')\n",
        "  plt.xlabel('Sample')\n",
        "  plt.ylabel('Amplitude')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig(os.path.join(base_dir, \"seismic_data_comparison.png\"))\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "5vGtx7hugJCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Main Function\n",
        "The `main` function orchestrates the complete workflow for processing seismic data, generating synthetic data, combining datasets, and training a model detect seismic events. It integrates data preparation, visualization and machinelearning in a sequential and modular manner.\n",
        "\n",
        "##Steps and functionality\n",
        "1. **Configuration Setup**\n",
        "Defines a configuration dictionary `config` with parameters for:\n",
        "\n",
        "  `window_size`: Size of the data window for training.\n",
        "\n",
        "  `step`: Step size for sliding window operations.\n",
        "\n",
        "  `sample_rate`: Default sampling rate (Hz).\n",
        "\n",
        "  `num_samples`: Number of samples in fallback synthetic data.\n",
        "\n",
        "2. **Output Directory Creation**\n",
        "Calls `create_output_directory` to create a directory for saving all outputs, including plots and model files.\n",
        "\n",
        "3. **Fetching real earthquake data**\n",
        "Defines the earthquake's origin time using `UTCDateTime` and fetches data using `fetch_iris_data` with:\n",
        "\n",
        "  Seismic network, station and channel information.\n",
        "\n",
        "  `origin_time` for temporal alignment.\n",
        "\n",
        "  Duration of 120 seconds.\n",
        "\n",
        "If the data fetch is successful:\n",
        "\n",
        "   Processes the data stream using `process_stream` to filter the signal and compute the charcteristic function (`real_cft`).\n",
        "\n",
        "  Extracts raw data and its sampling rate.\n",
        "\n",
        "If no data is fetched:\n",
        "\n",
        "  Initializes fallback data as a zero-filled array with length `num_samples` and defualt `sample_rate`.\n",
        "\n",
        "  Sets `real_cft` to `None`.\n",
        "\n",
        "4. **Synthetic Data Generation**\n",
        "Calls `generate_synthetic_data` to create synthetic seismic data with parameters:\n",
        "\n",
        "  Length equal to the rel data.\n",
        "\n",
        "  Sampling rate.\n",
        "\n",
        "  Event duration (2 seconds) and noise level(0.1)\n",
        "\n",
        "Outputs include:\n",
        "\n",
        "  `synthetic_data`: Generated data.\n",
        "\n",
        "  `synthetic_events`: List of synthetic event times.\n",
        "\n",
        "  `synthetic_event_location`: Location of synthetic events in the data.\n",
        "\n",
        "5. **Combining Datasets**:\n",
        "Adds `real_data` and `synthetic_data` element wise to create `combined_data`, simulating real-world scenarios where noise and real seismic signals overlap.\n",
        "\n",
        "6. **Creating Windows and labels**\n",
        "Uses `create_enhanced_windows` to split `combined_data` into sliding widows with:\n",
        "\n",
        "  Defines `window_size` and `step` from the configuration\n",
        "\n",
        "  Event locations (`synthetic_event_location`) for labelling.\n",
        "\n",
        "  The real signal's characteristic function (`real_cft`) to refine labelling.\n",
        "\n",
        "7. **Model Traing and Evaluation**\n",
        "Calls `train_evaluate_model` to train the model using:\n",
        "\n",
        "  Features (`X`) and labels (`y`) from the windowed data.\n",
        "\n",
        "  Outputs the trained model and cross-validation scores.\n",
        "\n",
        "8. **Visualization**\n",
        "Generates a plot comparing `real_data`, `synthetic_data` and `combined_data` using `plot_seismic_data` saving it in the `base_dir`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T1imCmmh3AJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Configuration\n",
        "    config = {\n",
        "        'window_size': 250,\n",
        "        'step': 25,\n",
        "        'sample_rate': 100,\n",
        "        'num_samples': 25000\n",
        "    }\n",
        "\n",
        "    # Create output directory\n",
        "    base_dir = create_output_directory()\n",
        "\n",
        "    # Fetch real earthquake data\n",
        "    origin_time = UTCDateTime(\"2015-08-11T16:22:15.200000\")\n",
        "    real_stream = fetch_iris_data(\n",
        "        network='YS', station='BAOP', location='',\n",
        "        channel='BHZ', origin_time=origin_time, duration=120\n",
        "    )\n",
        "\n",
        "    # Process real and synthetic data\n",
        "    if real_stream is not None:\n",
        "        filtered_real_stream, real_cft = process_stream(real_stream)\n",
        "        real_data = filtered_real_stream[0].data\n",
        "        sample_rate = real_stream[0].stats.sampling_rate\n",
        "    else:\n",
        "        # Fallback to default values if no real data\n",
        "        real_data = np.zeros(config['num_samples'])\n",
        "        sample_rate = config['sample_rate']\n",
        "        real_cft = None\n",
        "\n",
        "    # Generate synthetic data\n",
        "    synthetic_data, synthetic_events, synthetic_event_locations = generate_synthetic_data(\n",
        "        len(real_data),\n",
        "        sample_rate,\n",
        "        event_duration=2,\n",
        "        noise_level=0.1\n",
        "    )\n",
        "\n",
        "    # Combine datasets\n",
        "    combined_data = real_data + synthetic_data\n",
        "\n",
        "    # Create windows and labels\n",
        "    X, y = create_enhanced_windows(\n",
        "        combined_data,\n",
        "        config['window_size'],\n",
        "        config['step'],\n",
        "        synthetic_event_locations,\n",
        "        real_cft\n",
        "    )\n",
        "\n",
        "    # Train and evaluate model\n",
        "    trained_model, cv_scores = train_and_evaluate_model(X, y, base_dir)\n",
        "    plot_seismic_data(real_data, synthetic_data, combined_data, base_dir)\n",
        "\n",
        "    print(\"\\nModel training and evaluation complete. Check the output directory for visualizations.\")"
      ],
      "metadata": {
        "id": "4xi66A8AlpCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "TSjpEL5okayU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}