{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM08zDKGkB93qtZVUKldtiR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedamusk/final-year/blob/main/weontosomething22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install obspy tensorflow  matplotlib"
      ],
      "metadata": {
        "id": "WdHd8oF_-29J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfzX02XK-1Db"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from obspy import UTCDateTime, Stream, Trace\n",
        "from obspy.clients.fdsn import Client\n",
        "from obspy.signal.trigger import classic_sta_lta\n",
        "from obspy.signal.filter import bandpass\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "def fetch_iris_data(network, station, location, channel, start_time, end_time):\n",
        "    \"\"\"\n",
        "    Fetch seismic data from IRIS database\n",
        "    \"\"\"\n",
        "    try:\n",
        "        client = Client(\"IRIS\")\n",
        "        stream = client.get_waveforms(network, station, location, channel, start_time, end_time)\n",
        "        return stream\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data from IRIS: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_synthetic_data(num_samples, sample_rate, event_duration, noise_level):\n",
        "    \"\"\"\n",
        "    Generate synthetic seismic data for testing and validation\n",
        "    \"\"\"\n",
        "    time = np.arange(num_samples) / sample_rate\n",
        "    background = np.random.normal(0, noise_level, num_samples)\n",
        "\n",
        "    # Create multiple events for more complex training\n",
        "    num_events = np.random.randint(1, 4)\n",
        "    events = np.zeros(num_samples)\n",
        "    event_locations = []\n",
        "\n",
        "    for _ in range(num_events):\n",
        "        event_start = np.random.randint(num_samples // 8, num_samples * 7 // 8)\n",
        "        event_end = event_start + int(event_duration * sample_rate)\n",
        "        event_locations.append((event_start, event_end))\n",
        "\n",
        "        # Create event with varying frequency and amplitude\n",
        "        freq = np.random.uniform(3, 8)\n",
        "        amp = np.random.uniform(0.8, 1.2)\n",
        "        decay = np.random.uniform(0.1, 0.3)\n",
        "\n",
        "        event = np.sin(2 * np.pi * freq * (time[event_start:event_end] - time[event_start])) * \\\n",
        "                amp * np.exp(-(time[event_start:event_end] - time[event_start]) / decay)\n",
        "        events[event_start:event_end] = event\n",
        "\n",
        "    data = background + events\n",
        "    return data, events, event_locations\n",
        "\n",
        "def create_windows(data, window_size, step):\n",
        "    \"\"\"\n",
        "    Create sliding windows for RNN input with overlap\n",
        "    \"\"\"\n",
        "    windows = []\n",
        "    labels=[]\n",
        "\n",
        "    for i in range(0, len(data) - window_size + 1, step):\n",
        "        window = data[i:i + window_size]\n",
        "        # Add feature engineering\n",
        "        window_features = np.column_stack((\n",
        "            window,  # Raw signal\n",
        "            np.abs(window),  # Absolute amplitude\n",
        "            np.gradient(window),  # First derivative\n",
        "            np.gradient(np.gradient(window))  # Second derivative\n",
        "        ))\n",
        "        windows.append(window_features)\n",
        "\n",
        "        if len(window)==window_size:\n",
        "          labels.append(window.mean())\n",
        "    return np.array(windows), np.array(labels)\n",
        "\n",
        "def build_enhanced_rnn_model(input_shape):\n",
        "    \"\"\"\n",
        "    Build an enhanced RNN model with multiple LSTM layers and additional features\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # First Bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(128, return_sequences=True), input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Second Bidirectional LSTM layer\n",
        "        Bidirectional(LSTM(96, return_sequences=True)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Third LSTM layer\n",
        "        LSTM(64),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Dense layers for classification\n",
        "        Dense(32, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Use Adam optimizer with custom learning rate\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', 'Precision', 'Recall']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def process_stream(stream, freqmin=0.5, freqmax=20):\n",
        "    \"\"\"\n",
        "    Process a seismic stream with enhanced filtering\n",
        "    \"\"\"\n",
        "    processed_stream = stream.copy()\n",
        "    processed_stream.detrend('linear')\n",
        "    processed_stream.taper(max_percentage=0.05)\n",
        "    processed_stream.filter('bandpass', freqmin=freqmin, freqmax=freqmax,\n",
        "                          corners=4, zerophase=True)\n",
        "    return processed_stream\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, predictions):\n",
        "  binary_predictions=(predictions>0.5).astype(int)\n",
        "\n",
        "  accuracy=accuracy_score(y_test, binary_predictions)\n",
        "  precision=precision_score(y_test, binary_predictions)\n",
        "  recall=recall_score(y_test, binary_predictions)\n",
        "  f1=f1_score(y_test, binary_predictions)\n",
        "\n",
        "  report=classification_report(y_test, binary_predictions)\n",
        "\n",
        "  cm=confusion_matrix(y_test, binary_predictions)\n",
        "\n",
        "  print(\"\\n=== Model Performance Metrics ===\")\n",
        "  print(f\"Accuracy:{accuracy:.4f}\")\n",
        "  print(f\"Precision:{precision:.4f}\")\n",
        "  print(f\"Recall:{recall:.4f}\")\n",
        "  print(f\"F1 Score:{f1:.4f}\")\n",
        "\n",
        "  print(\"\\n=== Classification report ===\")\n",
        "  print(report)\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.ylabel('True Label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "\n",
        "  return accuracy, precision, recall, f1\n",
        "\n",
        "def main(use_real_data=True):\n",
        "    # Configuration parameters\n",
        "    config = {\n",
        "        'window_size': 200,\n",
        "        'step': 20,\n",
        "        'sample_rate': 100,\n",
        "        'sta_length': 50,  # 0.5 seconds\n",
        "        'lta_length': 500,  # 5 seconds\n",
        "        'threshold': 1.5,\n",
        "        'num_samples': 20000\n",
        "    }\n",
        "\n",
        "    if use_real_data:\n",
        "        # Fetch real data from IRIS\n",
        "        end_time = UTCDateTime.now()\n",
        "        start_time = end_time - 3600  # One hour\n",
        "\n",
        "        stream = fetch_iris_data('YS', 'BAOP', '', 'BHZ', start_time, end_time)\n",
        "        if stream is None:\n",
        "            print(\"Falling back to synthetic data...\")\n",
        "            use_real_data = False\n",
        "        else:\n",
        "            filtered_stream = process_stream(stream)\n",
        "\n",
        "    if not use_real_data:\n",
        "        # Generate synthetic data with multiple events\n",
        "        synthetic_data, true_events, event_locations = generate_synthetic_data(\n",
        "            config['num_samples'],\n",
        "            config['sample_rate'],\n",
        "            event_duration=2,\n",
        "            noise_level=0.1\n",
        "        )\n",
        "\n",
        "        trace = Trace(data=synthetic_data)\n",
        "        trace.stats.starttime = UTCDateTime(\"2021-01-01T00:00:00\")\n",
        "        trace.stats.delta = 1.0/config['sample_rate']\n",
        "        trace.stats.channel = 'SHZ'\n",
        "        filtered_stream = Stream([trace])\n",
        "        filtered_stream = process_stream(filtered_stream)\n",
        "\n",
        "    # Create training data\n",
        "    X, y= create_windows(\n",
        "        filtered_stream[0].data,\n",
        "        config['window_size'],\n",
        "        config['step']\n",
        "    )\n",
        "\n",
        "    if use_real_data:\n",
        "        # For real data, use STA/LTA triggers as initial labels\n",
        "        triggers, sta_lta = sta_lta_detection(\n",
        "            filtered_stream,\n",
        "            config['sta_length'],\n",
        "            config['lta_length'],\n",
        "            config['threshold']\n",
        "        )\n",
        "\n",
        "        y=(y>config['threshold']).astype(int)\n",
        "    else:\n",
        "        y=(y>0).astype(int)\n",
        "\n",
        "    print(f\"X shape:{X.shape}\")\n",
        "    print(f\"y shape: {y.shape}\")\n",
        "    # Prepare data for training\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Normalize data\n",
        "    mean = X_train.mean(axis=(0, 1), keepdims=True)\n",
        "    std = X_train.std(axis=(0, 1), keepdims=True)\n",
        "    X_train = (X_train - mean) / std\n",
        "    X_test = (X_test - mean) / std\n",
        "\n",
        "    # Build and train model\n",
        "    input_shape = (config['window_size'], X.shape[2])\n",
        "    model = build_enhanced_rnn_model(input_shape)\n",
        "\n",
        "    # Add callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    accuracy, precision, recall, f1=evaluate_model(model, X_test, y_test, predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Plot results\n",
        "    plot_results(filtered_stream, predictions, config, use_real_data, metrics={'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1})\n",
        "    plot_training_history(history)\n",
        "\n",
        "def plot_results(stream, predictions, config, use_real_data, metrics=None):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot original data\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(stream[0].data)\n",
        "    if metrics:\n",
        "      plt.title(f\"Original Seismic Data\\nAccuracy:{metrics['accuracy']:.4f},\"\n",
        "      f\"Precision:{metrics['precision']:.4f},\"\n",
        "      f\"Recall{metrics['recall']:.4f},\"\n",
        "      f\"F1:{metrics['f1']:.4f}\")\n",
        "    else:\n",
        "      plt.title(\"Original Seismic Data\")\n",
        "    plt.xlabel(\"Samples\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "\n",
        "    # Plot predictions\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(predictions)\n",
        "    plt.title(\"Detection Predictions\")\n",
        "    plt.xlabel(\"Windows\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "\n",
        "    plt.subplot(4,1,3)\n",
        "    plt.plot((predictions>0.5).astype(int))\n",
        "    plt.title(\"Binary Predictions (Threshold=0.5)\")\n",
        "    plt.xlabel(\"Windows\")\n",
        "    plt.ylabel('Detection')\n",
        "\n",
        "    # Plot spectrogram\n",
        "    plt.subplot(3, 1, 3)\n",
        "    stream.spectrogram(show=False)\n",
        "    plt.title(\"Spectrogram\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_training_history(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot(history.history['Precision'], label='Precision')\n",
        "    plt.plot(history.history['Recall'], label='Recall')\n",
        "    plt.title('Precision and Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(use_real_data=True)"
      ]
    }
  ]
}